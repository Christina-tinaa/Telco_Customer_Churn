---
title: "Neural Network Analysis of IBM Telco Customer Churn "
author: "CHRISTINA THOMPSON ACQUAH"
date: " "
output:
  html_document: 
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 3
    fig_height: 3
editor_options: 
  chunk_output_type: inline
---

```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  font-weight: bold;
  color: navy;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 18px;
  font-family: system-ui;
  color: navy;
  text-align: center;
  font-weight: bold;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
  font-weight: bold;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: center;
    font-weight: bold;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
    font-weight: bold;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```

```{r setup, include=FALSE}
# code chunk specifies whether the R code, warnings, and output 
# will be included in the output files.
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("tidyverse")) {
   install.packages("tidyverse")
library(tidyverse)
}
if (!require("GGally")) {
   install.packages("GGally")
library(GGally)
}
if (!require("glmnet")) {
   install.packages("glmnet")
library(glmnet)
}
if (!require("caret")) {
   install.packages("caret")
library(caret)
}
if (!require("MASS")) {
   install.packages("MASS")
library(MASS)
}
if (!require("mlbench")) {
   install.packages("mlbench")
library(mlbench)
}
if (!require("pROC")) {
   install.packages("pROC")
library(pROC)
}
if (!require("plotly")) {
   install.packages("plotly")
library(plotly)
}
if (!require("pander")) {
   install.packages("pander")
library(pander)
}
### 
knitr::opts_chunk$set(echo = TRUE,       
                      warning = FALSE,    
                      results = TRUE,    
                      message = FALSE,
                      comment = NA
                      )  
```

# . Introduction

The IBM Telco Customer Churn dataset comprises roughly 7,000 residential customers, each described by demographic (e.g.gender, senior citizen status), service (Internet, streaming, security add‑ons), contract and payment details, and billing information. Crucially, it provides both a numeric outcome (MonthlyCharges) and a categorical outcome (Churn), making it an ideal testbed for applying neural networks to simultaneous regression and classification tasks.

## Key Research Questions
To guide our analysis, we formulate two practical questions:

1. Regression: Which combination of customer demographics, service usage, and billing features best predicts a customer’s MonthlyCharges?

2. Classification: Which combination of those same features best discriminates between customers who churn and those who remain?

By addressing these questions, we aim to provide actionable insights that can aid in early detection and prevention strategies.

##  Data Description & Assessment

The IBM Telco Customer Churn dataset contains 7,043 customer records with the following structure:

-Demographics: gender, SeniorCitizen, Partner, Dependents

-Account & Service: tenure, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies

-Contract & Billing: Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges

-Response Variables: Churn (Yes/No), MonthlyCharges (numeric)


All 7,043 customer records arrived perfectly intact zero missing values across 33 columns—including demographics (gender, SeniorCitizen), a full suite of service flags (InternetService, StreamingTV, TechSupport, and more), contract details, payment methods, and billing metrics.

The churn indicator shows that 1,869 of 7,043 customers have left about 26% churn versus 74% retention; a healthy split for training both classification and regression networks.  Our numeric target, MonthlyCharges, runs from $18.25 up to $118.75, with a median near $70 and a mean around $65.  That wide spread means we need a model capable of capturing non‑linear pricing patterns, and there’s plenty of variation for a regression net to learn meaningful differences.

Because the data are already complete, we can skip imputation entirely and jump straight into feature engineering.  With a clean, fully understood dataset in hand, we’re ready to proceed with our EDA and the neural‑network implementations.

```{r,echo=TRUE}
library(readxl)
library(tidyverse)

url  <- "https://raw.githubusercontent.com/Christina-tinaa/Telco_Customer_Churn/main/Telco_customer_churn.xlsx"
tmp  <- tempfile(fileext = ".xlsx")          # makes a throw-away local file
download.file(url, tmp, mode = "wb")         # mode = "wb" is vital on Windows
df <- read_excel(tmp)





# 2. Read the .xlsx
#df <- readxl::read_excel(
 # "https://raw.githubusercontent.com/Christina-tinaa/Telco_Customer_Churn/main/Telco_customer_churn.xlsx")
# 3. Inspect structure and missing values
#glimpse(df)
#df %>% summarise(across(everything(), ~ sum(is.na(.))))

# 4. Examine the two response distributions
#table(df$Churn)
#head(df$MonthlyCharges)


```

# . Exploratory Data Analysis (EDA)

Before we dive into modeling, it is essential to understand the key patterns, distributions, and relationships in our data. In this section, we will:

-Examine the distribution of our numeric target (MonthlyCharges) and identify any skewness or outliers.

-Assess class balance in our categorical outcome (Churn).

-Compare group summaries to see how billing differs between churners and non‑churners.

-Visualize bivariate relationships, such as how customer tenure interacts with monthly charges by churn status.

These EDA steps will guide our feature engineering and help us choose appropriate neural‑network architectures.


## Distribution of Monthly Charges
The histogram shows how customers’ monthly bills are spread across the range $18–$120. There’s a pronounced spike in the lowest bin ($18–$25), reflecting a large segment on minimal plans. Beyond that, the distribution fans out fairly evenly between $30 and $100, with a slight bulge around $70–$80. This tells us two things: most customers either choose a basic, low cost package or a mid to high tier bundle, and there’s enough variation in billing to train a regression network that can pick up non‑linear pricing patterns.

```{r rename-and-histogram, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)

# 1. Rename columns to syntactic names
df <- df %>% rename(
  longitude           = Longitude,
  latitude            = Latitude,
  gender              = Gender,
  senior_citizen      = `Senior Citizen`,   # "Yes"/"No"
  partner             = Partner,
  dependents          = Dependents,
  tenure_months       = `Tenure Months`,
  phone_service       = `Phone Service`,
  multiple_lines      = `Multiple Lines`,
  internet_service    = `Internet Service`,
  online_security     = `Online Security`,
  online_backup       = `Online Backup`,
  device_protection   = `Device Protection`,
  tech_support        = `Tech Support`,
  streaming_tv        = `Streaming TV`,
  streaming_movies    = `Streaming Movies`,
  contract            = Contract,
  paperless_billing   = `Paperless Billing`,
  payment_method      = `Payment Method`,
  monthly_charges     = `Monthly Charges`,
  total_charges       = `Total Charges`,    # character → numeric
  churn_label         = `Churn Label`,
  churn_value         = `Churn Value`,
  churn_score         = `Churn Score`,
  cltv                = CLTV,
  churn_reason        = `Churn Reason`
)




# 2. Plot histogram of MonthlyCharges
ggplot(df, aes(x = monthly_charges)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  labs(
    title = "Distribution of Monthly Charges",
    x     = "Monthly Charges (USD)",
    y     = "Count"
  ) +
  theme_minimal()
```

## Churn Rate Bar Chart
The bar chart lays out the class balance for our classification task: about 5,174 customers did not churn, while 1,869 did. That translates to roughly 74% retention and 26% churn a healthy imbalance that neural nets and baseline classifiers will need to address (for example, by weighting or resampling) to avoid simply predicting the majority “No churn” class.

```{r bar chart, message=FALSE, warning=FALSE}
# Bar chart of churn
ggplot(df, aes(x = factor(churn_label), fill = factor(churn_label))) +
  geom_bar() +
  scale_fill_manual(values = c("skyblue","salmon")) +
  labs(title = "Churn Rate", x = "Churn", y = "Count") +
  theme(legend.position = "none")
```

## Monthly Charges by Churn Group (Summary Table)
Grouping customers by churn status reveals a clear billing gap:

Non‑churners (n = 5,174) pay on average $61.27 per month (median $64.43) with a standard deviation of $31.09.

Churners (n = 1,869) have a higher mean of $74.44 (median $79.65) and slightly lower spread (sd = $24.67).

This confirms that higher monthly bills correlate with a greater likelihood of leaving—an insight our classification models must leverage to separate “at‑risk” customers from the rest.

```{r group , message=FALSE, warning=FALSE}
df %>% 
  group_by(churn_label) %>% 
  summarise(
    n      = n(),
    mean   = mean(monthly_charges),
    median = median(monthly_charges),
    sd     = sd(monthly_charges)
  )

```

## Tenure vs Monthly Charges by Churn Status
In the tenure‐vs‐charges scatter, churners (teal) cluster almost exclusively at low tenures (most within the first 20 months) and tend to incur higher charges during that initial period. Non‑churners (salmon) span the full tenure range up to 72 months, with their average charges rising more gradually. The diverging LOESS lines show that early tenure combined with high billing is a strong churn signal exactly the kind of non‑linear interaction a neural network can learn to flag.

```{r tenure-vs-charges, fig.width=7, fig.height=5, message=FALSE, warning=FALSE}
library(ggplot2)

ggplot(df, aes(x = `tenure_months`, y = monthly_charges, color = churn_label)) +
  geom_point(alpha = 0.4, size = 1.5) +
  geom_smooth(method = "loess", se = FALSE, linetype = "dashed") +
  labs(
    title = "Customer Tenure vs Monthly Charges by Churn Status",
    x     = "Tenure (months)",
    y     = "Monthly Charges (USD)",
    color = "Churn Status"
  ) +
  theme_minimal(base_size = 14)

```

# . Feature Engineering

Before training our neural networks, we must transform the raw Telco data into a machine ready format. First, we convert every Yes/No service and churn flag into a binary 0/1 encoding, preserving their simple two state meaning without bloating the input dimension. Next, we convert the total_charges field from text into a numeric type and impute any stray missing values using its median. Finally, we standardize all numeric predictors centering to zero mean and scaling to unit variance so that features on different scales (tenure in months vs. monthly charges in dollars) contribute evenly during gradient based training. With the data fully encoded, imputed, and scaled—and a 70/30 train/test split in place we’re ready to build and evaluate our suite of neural models.

```{r}
library(dplyr)
library(caret)

# 1. Binary‐encode all Yes/No flags
df2 <- df %>%
  mutate(across(
    c(partner, dependents, phone_service, multiple_lines,
      online_security, online_backup, device_protection, tech_support,
      streaming_tv, streaming_movies, paperless_billing, churn_label),
    ~ as.integer(. == "Yes")
  ))

# 2. Convert total_charges to numeric and impute NAs
df2 <- df2 %>%
  mutate(
    total_charges = as.numeric(total_charges),
    total_charges = if_else(
      is.na(total_charges),
      median(total_charges, na.rm = TRUE),
      total_charges
    )
  )
stopifnot(sum(is.na(df2$total_charges)) == 0)

# 3. Center and scale numeric predictors
num_vars <- c("tenure_months", "monthly_charges", 
              "total_charges", "churn_value", 
              "churn_score", "cltv")

preproc <- preProcess(df2[num_vars], method = c("center", "scale"))
df2[num_vars] <- predict(preproc, df2[num_vars])

# 4. Train/test split (70/30 stratified on churn_label)
set.seed(123)
train_idx <- createDataPartition(df2$churn_label, p = 0.70, list = FALSE)
train_nn  <- df2[train_idx, ]
test_nn   <- df2[-train_idx, ]

cat("Training set:", nrow(train_nn), "rows\n",
    "Test set:    ", nrow(test_nn),  "rows\n")


```

# . Summary of Baseline Methods

Before building neural networks, it helps to recall how classical algorithms tackle our two questions:

**Linear Regression**

Fits a straight line (or multi‑dimensional hyperplane) to predict monthly_charges by minimizing squared error. It is fast and interpretable but cannot capture non‑linearities or feature interactions without manual transformations.

**Logistic Regression**

Models the log‑odds of churn as a linear function of the inputs, providing calibrated probabilities and clear feature‐weight interpretations. Like its linear counterpart, it struggles when the decision boundary is strongly non‑linear.

*Support Vector Machine (SVM)*

Uses kernels (e.g. RBF) to project data into higher dimensions where a linear separator can distinguish churners from non‑churners or fit a regression “tube” around monthly_charges. SVMs can capture complex boundaries but require careful kernel and parameter tuning, and they scale poorly to very large datasets.

**CART (Classification and Regression Trees)**

Builds a decision‐tree by recursively splitting on the most informative features. CART naturally handles non‑linear relationships and interactions without feature engineering, and yields easy‐to‐interpret rules, but single trees tend to overfit and can be unstable.

**Ensembles (Bagging & Random Forest)**

Aggregate many CART models (bagging) or introduce random feature subsampling (random forest) to reduce variance and improve generalization. These methods often outperform single trees on both regression and classification tasks, at the cost of model interpretability.

These baseline methods provide useful performance and interpretability benchmarks. Our neural‑network models will be compared against them to gauge whether the added flexibility of deep architectures yields meaningful gains.


# .	Implementation of Neural Networks

With our features encoded and scaled, and the data split into training and test sets, we now turn to constructing and evaluating three neural‐network architectures. First, we establish perceptron baselines—a single linear unit for regression and a single logistic unit for classification—to gauge the basic predictive signal in our engineered features. Next, we’ll extend to multilayer perceptrons (MLPs) with hidden layers and non‐linear activations, tuning key hyperparameters to capture complex interactions. Throughout, we will measure performance using appropriate metrics (RMSE/MAE/R² for regression; accuracy, sensitivity, specificity, and AUC for classification), and compare neural models against our classical baselines to determine the practical value of going “deep.”

## . Perceptron Regression

The plot shows the perceptron’s mean‐squared error (MSE) on the training set as gradient‐descent proceeds through 200 epochs. We start up near an MSE of 1.0, and the curve falls smoothly to about 0.35 by epoch 200. That steady decline tells us the model is learning useful weight updates at each step. Because the curve is still gently sloping downward at the end, the network hasn’t fully converged—extending training slightly or lowering the learning rate could eke out small additional gains, but the bulk of the error reduction happens in the first  100 epochs.


```{r perceptron-regression, message=FALSE, warning=FALSE}
# 1) Define perceptron.regression() from lecture
perceptron.regression <- function(X.train, y.train,
                                  learning.rate = 0.01,
                                  epochs = 200) {
  n_obs  <- nrow(X.train)
  n_feat <- ncol(X.train)
  w      <- rep(0, n_feat)
  b      <- 0
  errors <- numeric(epochs)
  for (epoch in seq_len(epochs)) {
    # linear prediction
    y_pred <- as.vector(X.train %*% w + b)
    # error
    e      <- y.train - y_pred
    # gradient‐descent update
    w      <- w + (learning.rate / n_obs) * t(X.train) %*% e
    b      <- b + (learning.rate / n_obs) * sum(e)
    # record MSE
    errors[epoch] <- mean(e^2)
  }
  list(weights = w, bias = b, errors = errors)
}

# 2) Prepare training and test data
features_reg <- c(
  "tenure_months", "total_charges",
  "partner", "dependents", "phone_service",
  "multiple_lines", "online_security", "online_backup",
  "device_protection", "tech_support", "streaming_tv",
  "streaming_movies", "paperless_billing"
)
X_train_r <- as.matrix(train_nn[, features_reg])
y_train_r <- train_nn$monthly_charges
X_test_r  <- as.matrix(test_nn[,  features_reg])
y_test_r  <- test_nn$monthly_charges

# 3) Train perceptron regression
model_pr <- perceptron.regression(
  X.train       = X_train_r,
  y.train       = y_train_r,
  learning.rate = 0.01,
  epochs        = 200
)

# 4) Plot training MSE over epochs
plot(
  model_pr$errors, type = "l",
  xlab = "Epoch", ylab = "Mean Squared Error",
  main = "Perceptron Regression: Training Error"
)
```


**Test Set**

The model’s predictions miss the true (scaled) monthly charges by about 0.60 units on average (RMSE), or by 0.51 units in absolute terms (MAE). An R² of 0.672 means the perceptron explains roughly 67 % of the variance in customers’ monthly bills. For such a simple, single‐layer model, capturing two‐thirds of variability is a solid baseline.

```{r test_set, message=FALSE, warning=FALSE}
# 5) Predict on test set
pr_preds <- as.vector(X_test_r %*% model_pr$weights + model_pr$bias)

# 6) Compute test metrics
rmse_pr <- sqrt(mean((pr_preds - y_test_r)^2))
mae_pr  <- mean(abs(pr_preds - y_test_r))
r2_pr   <- cor(pr_preds, y_test_r)^2

cat(
  "Test Set Performance:\n",
  "  RMSE =", round(rmse_pr,3), "\n",
  "  MAE  =", round(mae_pr,3),  "\n",
  "  R²   =", round(r2_pr,3),   "\n"
)
```
**Predicted vs. Actual Scatter**

In this scatterplot, each point is one customer’s standardized monthly charge. The dashed 45° line marks perfect forecasts.

Mid‑range mastery: Around the center, points hug the diagonal tightly, showing our perceptron nails most “typical” bills.

Tail variability: At the extremes , the spread widens high bill customers tend to be underpredicted and very low bill customers overpredicted revealing the perceptron’s linear limitations.

Stripe effect: Notice the vertical band at the far left (standardized –1.5): these are customers on the cheapest plan, all clustered at roughly the same low charge.

```{r plot, message=FALSE, warning=FALSE}
# 7) Scatter predicted vs actual
library(ggplot2)
df_pr <- data.frame(actual = y_test_r, predicted = pr_preds)
ggplot(df_pr, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Perceptron Regression: Predicted vs Actual",
    x     = "Actual Monthly Charges",
    y     = "Predicted Monthly Charges"
  ) +
  theme_minimal()

```

## Perceptron Classification

Our single layer perceptron correctly labels about 76% of customers—only a hair above the 74% you’d get by simply predicting “no churn” every time. It catches just 21.6% of actual churners but never cries wolf, with a 95.3% true‑negative rate. An AUC of 0.59 tells the same story: there’s real signal in tenure and billing, but a straight‑line boundary can’t separate churners from stayers very well. To boost sensitivity without sacrificing specificity, we’ll next turn to a deeper, non‑linear network.

```{r perceptron-classification, message=FALSE, warning=FALSE}
library(nnet)
library(caret)
library(pROC)
set.seed(123)

# 1) Define our predictor set (drop any leakage variables)
features_clf <- c(
  "tenure_months","monthly_charges","total_charges",
  "partner","dependents","phone_service","multiple_lines",
  "online_security","online_backup","device_protection",
  "tech_support","streaming_tv","streaming_movies",
  "paperless_billing"
)

# 2) Prepare training and test matrices
X_train_c <- as.matrix(train_nn[, features_clf])
y_train_c <- train_nn$churn_label

X_test_c  <- as.matrix(test_nn[,  features_clf])
y_test_c  <- test_nn$churn_label

# 3) Train the perceptron classifier
model_pc <- nnet(
  x     = X_train_c,
  y     = y_train_c,
  size  = 0,       # no hidden units
  skip  = TRUE,    # direct input→output connections
  linout = FALSE,  # logistic output by default
  trace = FALSE,
  maxit = 200
)

# 4) Predict probabilities on the test set
pc_probs <- predict(model_pc, X_test_c, type = "raw")
pc_preds <- ifelse(pc_probs > 0.5, 1L, 0L)

# 5) Compute confusion matrix and ROC/AUC
cm_pc  <- confusionMatrix(
  factor(pc_preds), 
  factor(y_test_c), 
  positive = "1"
)
roc_pc <- roc(
  response  = y_test_c,
  predictor = as.vector(pc_probs),
  levels    = c(0,1),
  direction = "<"
)
auc_pc <- auc(roc_pc)

# 6) Print out the key metrics
cat(
  "Perceptron Classification (nnet):\n",
  "  Accuracy   =", round(cm_pc$overall["Accuracy"],3), "\n",
  "  Sensitivity=", round(cm_pc$byClass["Sensitivity"],3), "\n",
  "  Specificity=", round(cm_pc$byClass["Specificity"],3), "\n",
  "  AUC        =", round(auc_pc,3), "\n"
)
```

## Multilayer Neural Networks

The single layer perceptron nails most of the linear signal in our data but struggles at the extremes and cannot pick up non‑linear interactions. To address this, we added one hidden layer of five ReLU neurons (plus direct skip connections) to create a small Multilayer Perceptron (MLP). This modest extension allows the network to learn curved decision boundaries and adjust predictions non‑linearly where simple weight sums fall short.

**Regression MLP**

 Building on our perceptron baseline, we trained a one‐hidden‐layer MLP (five neurons) on the same 4,931/2,112 split. The results were striking: RMSE fell from 0.600 to 0.229, MAE dropped from 0.514 to 0.142, and R² climbed from 0.672 to 0.947 more than doubling our regression power. In the Predicted vs Actual scatter, points now hug the 45° line uniformly: high bill underpredictions and the low‐bill stripe have all but disappeared. By learning simple non‑linear interactions among tenure, charges, and service flags, that single hidden layer explains nearly 95% of monthly charge variance.

```{r mlp-nnet, message=FALSE, warning=FALSE}
library(nnet)
library(caret)
library(pROC)
library(ggplot2)
set.seed(123)

# ---- 1) Regression MLP ----
f_mlp_r <- monthly_charges ~ tenure_months + total_charges +
            partner + dependents + phone_service + multiple_lines +
            online_security + online_backup + device_protection +
            tech_support + streaming_tv + streaming_movies +
            paperless_billing

# size=5 hidden units, skip=TRUE for direct input→output
model_mlp_r <- nnet(
  formula    = f_mlp_r,
  data       = train_nn,
  size       = 5,
  linout     = TRUE,
  skip       = TRUE,
  trace      = FALSE,
  maxit      = 200
)

# Predict on test set
pred_mlp_r <- predict(model_mlp_r, newdata = test_nn)

# Compute metrics
rmse_mlp_r <- sqrt(mean((pred_mlp_r - test_nn$monthly_charges)^2))
mae_mlp_r  <- mean(abs(pred_mlp_r - test_nn$monthly_charges))
r2_mlp_r   <- cor(pred_mlp_r, test_nn$monthly_charges)^2

cat("MLP Regression (1 hidden layer):\n",
    "  RMSE =", round(rmse_mlp_r,3), 
    "  MAE =", round(mae_mlp_r,3), 
    "  R²  =", round(r2_mlp_r,3), "\n\n")

# Plot Predicted vs Actual
df_mlp_r <- data.frame(
  actual    = test_nn$monthly_charges,
  predicted = pred_mlp_r
)
ggplot(df_mlp_r, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.4, color = "darkorange") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "MLP Regression: Predicted vs Actual",
    x     = "Actual Monthly Charges",
    y     = "Predicted Monthly Charges"
  ) +
  theme_minimal()
```

**Classification MLP**

Building on our perceptron classifier, we trained a one‑hidden‑layer MLP (five neurons) on the same 4,931/2,112 split. The results were underwhelming: accuracy dipped slightly (0.759 → 0.738), sensitivity collapsed (0.216 → 0.009) even as specificity climbed (0.953 → 0.998), and AUC fell from 0.590 to 0.504. In short, the ROC curve hugs the diagonal our MLP’s extra capacity couldn’t overcome the feature limitations and severe class imbalance. To boost churn detection meaningfully, we’ll need richer signals, targeted sampling strategies, or more sophisticated architectures.

```{r mlp-class, message=FALSE, warning=FALSE}
# ---- 2) Classification MLP ----
f_mlp_c <- churn_label ~ tenure_months + monthly_charges + total_charges +
            partner + dependents + phone_service + multiple_lines +
            online_security + online_backup + device_protection +
            tech_support + streaming_tv + streaming_movies +
            paperless_billing

model_mlp_c <- nnet(
  formula    = f_mlp_c,
  data       = train_nn,
  size       = 5,
  skip       = TRUE,
  linout     = FALSE,  # logistic output
  trace      = FALSE,
  maxit      = 200
)

# Predict probabilities
probs_mlp_c <- predict(model_mlp_c, newdata = test_nn, type = "raw")
preds_mlp_c <- ifelse(probs_mlp_c > 0.5, 1L, 0L)

# Compute confusion & ROC/AUC
cm_mlp_c <- confusionMatrix(
  factor(preds_mlp_c),
  factor(test_nn$churn_label),
  positive = "1"
)
roc_mlp_c <- roc(
  test_nn$churn_label,
  as.vector(probs_mlp_c),
  levels    = c(0,1),
  direction = "<"
)
auc_mlp_c <- auc(roc_mlp_c)

cat("MLP Classification (1 hidden layer):\n",
    "  Accuracy   =", round(cm_mlp_c$overall["Accuracy"],3), "\n",
    "  Sensitivity=", round(cm_mlp_c$byClass["Sensitivity"],3), "\n",
    "  Specificity=", round(cm_mlp_c$byClass["Specificity"],3), "\n",
    "  AUC        =", round(auc_mlp_c,3), "\n\n")



``` 

# .	Implementation Process

## Feature encoding and scaling

We began by recoding every Yes/No service flag into a 0/1 integer, converting total_charges into numeric (and imputing a handful of blanks with the median), then centering and scaling all continuous predictors (tenure, charges, CLTV) to unit variance. This ensures our neural nets—and every baseline model—see inputs on the same scale and converge smoothly during training.

```{r feature-engineering, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)

df2 <- df %>%
  # 1) Binary‐encode Yes/No flags
  mutate(across(
    c(partner, dependents, phone_service, multiple_lines,
      online_security, online_backup, device_protection, tech_support,
      streaming_tv, streaming_movies, paperless_billing, churn_label),
    ~ as.integer(. == "Yes")
  )) %>%
  # 2) Convert total_charges to numeric and impute
  mutate(total_charges = as.numeric(total_charges),
         total_charges = ifelse(is.na(total_charges),
                                median(total_charges, na.rm=TRUE),
                                total_charges))

# 3) Center & scale selected numeric columns
num_vars <- c("tenure_months", "monthly_charges", "total_charges",
              "churn_value", "churn_score", "cltv")
preproc  <- preProcess(df2[, num_vars], method = c("center","scale"))
df_ready <- df2
df_ready[num_vars] <- predict(preproc, df2[num_vars])
```

## Two-way data splitting

To assess generalization, we held out 30% of the data (2,112 rows) as an untouched test set, stratified on churn to preserve the 74/26 class balance. The remaining 70% (4,931 rows) formed our training set, on which all hyperparameter searches, model fitting, and cross‑validation were performed.

```{r}
library(caret)
set.seed(123)
train_idx <- createDataPartition(df_ready$churn_label,
                                 p = 0.70, list = FALSE)
train_nn  <- df_ready[train_idx, ]
test_nn   <- df_ready[-train_idx, ]
cat("Training rows:", nrow(train_nn),
    " Test rows:", nrow(test_nn), "\n")
```
## Hyperparameter tuning

**Regression MLP → size=20, decay=0.001**

The model that minimized out‑of‑sample RMSE used 20 hidden neurons with a light weight‑decay of 0.001. Twenty units give the network enough capacity to capture the non‑linear relationships in monthly charge data—such as tiered pricing and bundling effects—while the small decay term softly shrinks weights to prevent over‑fitting. Together, this combination struck the best balance between flexibility and regularization, yielding the lowest cross‑validated RMSE.

**Classification MLP → size=5, decay=0.01**

For churn prediction, the optimal model was far simpler: just 5 hidden neurons paired with a stronger decay of 0.01. Fewer units reduce the risk of memorizing noise—critical in our imbalanced churn setting—while the higher decay penalty aggressively discourages large weights, improving generalization. This leaner architecture delivered the highest cross‑validated AUC, indicating it best navigates the sensitivity‑specificity trade‑off without over‑fitting rare churn cases.


```{r hyperparameter, message=FALSE, warning=FALSE}

library(caret)

# 0) Define columns to tune on
reg_cols <- c("tenure_months","total_charges","partner","dependents",
              "phone_service","multiple_lines","online_security",
              "online_backup","device_protection","tech_support",
              "streaming_tv","streaming_movies","paperless_billing",
              "monthly_charges")
clf_cols <- c(reg_cols, "churn_label")

# 1) Drop rows with any NA in those columns
train_tune <- train_nn[complete.cases(train_nn[, clf_cols]), ]

# 2) Regression tuning (5‑fold CV, RMSE)
ctrl_reg <- trainControl(method = "cv", number = 5)
grid_reg <- expand.grid(size = c(5, 10, 20), decay = c(0.01, 0.001))

set.seed(123)
tune_reg <- train(
  monthly_charges ~ tenure_months + total_charges +
    partner + dependents + phone_service + multiple_lines +
    online_security + online_backup + device_protection +
    tech_support + streaming_tv + streaming_movies +
    paperless_billing,
  data      = train_tune,
  method    = "nnet",
  metric    = "RMSE",
  trControl = ctrl_reg,
  tuneGrid  = grid_reg,
  linout    = TRUE,
  skip      = TRUE,
  trace     = FALSE,
  maxit     = 200
)

# Print optimal regression hyperparameters
best_reg <- tune_reg$bestTune
cat(" Optimal regression tuning: size =", best_reg$size,
    " decay =", best_reg$decay, "\n\n")


# 3) Classification tuning (5‑fold CV, ROC)
# ensure churn_label is a two‐level factor
train_tune$churn_label <- factor(train_tune$churn_label,
                                 levels = c(0, 1),
                                 labels = c("No", "Yes"))

ctrl_clf <- trainControl(
  method          = "cv",
  number          = 5,
  classProbs      = TRUE,
  summaryFunction = twoClassSummary
)
grid_clf <- expand.grid(size = c(5, 10, 20), decay = c(0.01, 0.001))

set.seed(123)
tune_clf <- train(
  churn_label ~ tenure_months + monthly_charges + total_charges +
    partner + dependents + phone_service + multiple_lines +
    online_security + online_backup + device_protection +
    tech_support + streaming_tv + streaming_movies +
    paperless_billing,
  data      = train_tune,
  method    = "nnet",
  metric    = "ROC",
  trControl = ctrl_clf,
  tuneGrid  = grid_clf,
  skip      = TRUE,
  trace     = FALSE,
  maxit     = 200
)

# Print optimal classification hyperparameters
best_clf <- tune_clf$bestTune
cat("Optimal classification tuning: size =", best_clf$size,
    " decay =", best_clf$decay, "\n")


```

## Final model training

Armed with our optimal hyperparameters, we now refit each of the four neural architectures on the entire 70 % training set (train_tune), so they can “see” all available data before we evaluate on the hold‑out:

**Perceptron Regression**

A single‐layer linear net (size = 0, skip = TRUE) is retrained via gradient‐descent for up to 200 iterations. This model serves as our simplest baseline for predicting monthly charges.

**MLP Regression**

We fit the small multilayer net with 20 hidden neurons and decay = 0.001—the combination that minimized CV RMSE. With these settings, the net can flexibly learn non‑linear price tiers and bundling effects while the light‐decay penalty prevents over‑fitting.

**Perceptron Classification**

A single logistic unit (size = 0, skip = TRUE) is refit to predict churn. This yields a pure linear decision boundary, giving us a direct comparison point for our deeper nets.

**MLP Classification**

The churn MLP is retrained with 5 hidden neurons and decay = 0.01—the sweep’s best setting for CV AUC. Fewer units and a stronger decay keep the model conservative, guarding against memorizing the rare churn cases.

All four models are now trained on the full training set using their chosen hyperparameters. In the next section, we’ll generate predictions on the 2,112‑row hold‑out and report RMSE/MAE/R² for regression and Accuracy/Sensitivity/Specificity/AUC for classification, directly contrasting each neural net against our classical baselines.


```{r final-model, message=FALSE, warning=FALSE}

library(nnet)

# 1) Final Perceptron Regression (size = 0, skip = TRUE)
final_pr <- nnet(
  monthly_charges ~ tenure_months + total_charges + partner + dependents +
    phone_service + multiple_lines + online_security + online_backup +
    device_protection + tech_support + streaming_tv + streaming_movies +
    paperless_billing,
  data    = train_tune,
  size    = 0,
  linout  = TRUE,
  skip    = TRUE,
  trace   = FALSE,
  maxit   = 200
)

# 2) Final MLP Regression (size & decay from tune_reg$bestTune)
best_reg <- tune_reg$bestTune
final_mlp_r <- nnet(
  monthly_charges ~ tenure_months + total_charges + partner + dependents +
    phone_service + multiple_lines + online_security + online_backup +
    device_protection + tech_support + streaming_tv + streaming_movies +
    paperless_billing,
  data    = train_tune,
  size    = best_reg$size,
  decay   = best_reg$decay,
  linout  = TRUE,
  skip    = TRUE,
  trace   = FALSE,
  maxit   = 200
)

# 3) Final Perceptron Classification (size = 0, skip = TRUE)
final_pc <- nnet(
  churn_label ~ tenure_months + monthly_charges + total_charges + partner +
    dependents + phone_service + multiple_lines + online_security +
    online_backup + device_protection + tech_support + streaming_tv +
    streaming_movies + paperless_billing,
  data    = train_tune,
  size    = 0,
  skip    = TRUE,
  linout  = FALSE,
  trace   = FALSE,
  maxit   = 200
)

# 4) Final MLP Classification (size & decay from tune_clf$bestTune)
best_clf <- tune_clf$bestTune
final_mlp_c <- nnet(
  churn_label ~ tenure_months + monthly_charges + total_charges + partner +
    dependents + phone_service + multiple_lines + online_security +
    online_backup + device_protection + tech_support + streaming_tv +
    streaming_movies + paperless_billing,
  data    = train_tune,
  size    = best_clf$size,
  decay   = best_clf$decay,
  skip    = TRUE,
  linout  = FALSE,
  trace   = FALSE,
  maxit   = 200
)

```


##	Prediction and performance evaluation

**Regression**

Our simple perceptron regressor captured a respectable portion of billing variance (R²=0.811) but still missed the mark by nearly half a standard deviation (RMSE=0.431, MAE=0.353). Introducing twenty hidden neurons with light weight‑decay slashed those errors by more than half—now RMSE=0.212 and MAE=0.081—and boosted R² to an impressive 0.956. In plain terms, the MLP’s non‑linear transforms nail both typical bills and edge cases, cutting average prediction error from “almost three‑quarters of a pricing tier” down to “well under one tier.” For monthly‑charge regression, the MLP is a clear winner: its ability to learn curved pricing patterns more than doubles our predictive power compared to any linear model.

**Classification**

Our linear perceptron churn detector held its ground, achieving an AUC of 0.855 with 58.7% sensitivity, 89.0% specificity, and 81% accuracy. In contrast, the five‑neuron MLP slid backwards—accuracy dropped to 79.3%, sensitivity to 55.1%, specificity to 87.9%, and AUC fell to 0.842. In other words, the added non‑linear capacity actually diluted the churn signal rather than sharpened it. With our current feature set, a simple linear boundary provides the most robust trade‑off; extra network complexity only invites over‑fitting without improving discrimination

```{r}
# Prediction & Performance Evaluation

library(caret)
library(pROC)

# — Ensure test labels are factors with levels "No","Yes" —
test_nn$churn_label <- factor(test_nn$churn_label,
                              levels = c(0,1),
                              labels = c("No","Yes"))

# 1) Perceptron Regression
pred_pr <- predict(final_pr, newdata = test_nn)
rmse_pr <- RMSE(pred_pr, test_nn$monthly_charges)
mae_pr  <- MAE(pred_pr,  test_nn$monthly_charges)
r2_pr   <- cor(pred_pr, test_nn$monthly_charges)^2

# 2) MLP Regression
pred_mlp_r <- predict(final_mlp_r, newdata = test_nn)
rmse_mlp   <- RMSE(pred_mlp_r, test_nn$monthly_charges)
mae_mlp    <- MAE(pred_mlp_r,  test_nn$monthly_charges)
r2_mlp     <- cor(pred_mlp_r,    test_nn$monthly_charges)^2

# 3) Perceptron Classification
prob_pc <- predict(final_pc, newdata = test_nn, type = "raw")
pred_pc <- factor(ifelse(prob_pc > 0.5, "Yes", "No"),
                  levels = c("No","Yes"))
cm_pc   <- confusionMatrix(pred_pc, test_nn$churn_label, positive = "Yes")
roc_pc  <- roc(test_nn$churn_label, as.vector(prob_pc),
               levels = c("No","Yes"), direction = "<")
auc_pc  <- auc(roc_pc)

# 4) MLP Classification
prob_mlp_c <- predict(final_mlp_c, newdata = test_nn, type = "raw")
pred_mlp_c <- factor(ifelse(prob_mlp_c > 0.5, "Yes", "No"),
                     levels = c("No","Yes"))
cm_mlp_c   <- confusionMatrix(pred_mlp_c, test_nn$churn_label, positive = "Yes")
roc_mlp_c  <- roc(test_nn$churn_label, as.vector(prob_mlp_c),
                  levels = c("No","Yes"), direction = "<")
auc_mlp_c  <- auc(roc_mlp_c)

# 5) Print all metrics
cat(
  "Perceptron Reg → RMSE:", round(rmse_pr,3),
   " MAE:", round(mae_pr,3),
   " R²:", round(r2_pr,3), "\n",
  "MLP Reg       → RMSE:", round(rmse_mlp,3),
   " MAE:", round(mae_mlp,3),
   " R²:", round(r2_mlp,3), "\n\n",
  "Perceptron Clf → Acc:", round(cm_pc$overall["Accuracy"],3),
   " Sens:", round(cm_pc$byClass["Sensitivity"],3),
   " Spec:", round(cm_pc$byClass["Specificity"],3),
   " AUC:", round(auc_pc,3), "\n",
  "MLP Clf       → Acc:", round(cm_mlp_c$overall["Accuracy"],3),
   " Sens:", round(cm_mlp_c$byClass["Sensitivity"],3),
   " Spec:", round(cm_mlp_c$byClass["Specificity"],3),
   " AUC:", round(auc_mlp_c,3), "\n"
)

```
# . Comparisons with Base Models

By replacing the single linear unit with a 20‑neuron hidden layer, RMSE plunged from 0.431 to 0.212 and MAE from 0.353 to 0.081—driving R² from 81% to 95.6%—which shows that the MLP’s non‑linear transformations more than double our predictive power by capturing tiered pricing, bundling effects, and outliers that the perceptron missed.

```{r compare, message=FALSE, warning=FALSE}
library(tibble)
library(knitr)

# Regression comparison
reg_tbl <- tibble(
  Model = c("Perceptron‑Reg", "MLP‑Reg"),
  RMSE  = c(round(rmse_pr,   3), round(rmse_mlp_r, 3)),
  MAE   = c(round(mae_pr,    3), round(mae_mlp_r, 3)),
  R2    = c(round(r2_pr,     3), round(r2_mlp_r,  3))
)

kable(
  reg_tbl,
  caption = "Test‑set Performance: Perceptron vs MLP (Regression)"
)

```

**Perceptron-vs-mlp-classification**

Across every key metric—accuracy (81% vs. 79.3%), sensitivity (58.7% vs. 55.1%), specificity (89.0% vs. 87.9%), and AUC (0.855 vs. 0.842)—the simple logistic perceptron outperforms the small MLP, proving that added non‑linear hidden units only dilute the churn signal by over‑fitting scarce positive cases; in this setting, simplicity wins, and the perceptron remains our most robust classifier.

```{r perceptron-vs-mlp-clf, message=FALSE, warning=FALSE}
library(tibble)
library(knitr)

# Classification comparison
clf_tbl <- tibble(
  Model       = c("Perceptron‑Clf", "MLP‑Clf"),
  Accuracy    = c(
                   round(cm_pc$overall["Accuracy"],    3),
                   round(cm_mlp_c$overall["Accuracy"],3)
                 ),
  Sensitivity = c(
                   round(cm_pc$byClass["Sensitivity"],    3),
                   round(cm_mlp_c$byClass["Sensitivity"],3)
                 ),
  Specificity = c(
                   round(cm_pc$byClass["Specificity"],    3),
                   round(cm_mlp_c$byClass["Specificity"],3)
                 ),
  AUC         = c(
                   round(auc_pc,    3),
                   round(auc_mlp_c, 3)
                 )
)

kable(
  clf_tbl,
  caption = "Test‑set Performance: Perceptron vs MLP (Classification)"
)

```

## Insight

**Non‑linear power for pricing**: The 20‑neuron MLP slashed RMSE from 0.431 to 0.212, MAE from 0.353 to 0.081, and lifted R² from 81% to 95.6%. Its hidden layer captures complex billing patterns—tiered pricing, bundling, and outliers—that a linear model simply can’t.

**Simplicity for churn**: For predicting churn, the single‑unit perceptron (AUC=0.855) outperforms the small MLP (AUC=0.842) on every metric. Extra non‑linear capacity only over‑fits the rare churn cases instead of revealing meaningful structure—so a lean, linear boundary is the most reliable.

**Feature gaps remain**: While MLPs excel at regression, our classification task suffers from weak, mostly linear signals. To unlock deeper nets for churn, we must enrich the data adding usage logs, sentiment scores, or support call metadata to supply the non‑linear patterns that complex models need.

# . Recommendation and Conclusion

In summary, our experiments show that a 20‑neuron MLP should power all billing forecasts—its hidden layer halved RMSE, slashed MAE, and boosted R² to 95.6%—while a simple logistic perceptron remains the most reliable churn model (AUC=0.855) until richer, non‑linear customer signals are available. Moving forward, deploy these two models in production, invest in enhanced usage, sentiment, and support‑call features, apply targeted sampling or cost‑sensitive training to address imbalance, and establish continuous monitoring and retraining to ensure sustained performance as data patterns evolve.


